{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from nnsight import NNsight\n",
    "from utils import load_gemma_sae\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "lm = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\").to(torch.bfloat16).cuda()\n",
    "# model = NNsight(lm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tok_strs = np.array([tokenizer.decode([tok_id]).replace(' ', '·').replace('\\n', '⤶') for tok_id in range(256000)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noa_tools import reload_module\n",
    "reload_module('noa_tools')\n",
    "from noa_tools import register_hook, remove_hooks, reload_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER = 0\n",
    "# L0=43\n",
    "\n",
    "# sae = load_gemma_sae('att', filename=f'layer_{LAYER}/width_65k/average_l0_{L0}').to(torch.bfloat16).cuda()\n",
    "\n",
    "# sae.W_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "reload_module('utils')\n",
    "from utils import CustomBreakError, register_sae\n",
    "from noa_tools import clear_cache\n",
    "\n",
    "NUM_DOCS = 10000\n",
    "BATCH_SIZE = 300\n",
    "\n",
    "num_batches= NUM_DOCS // BATCH_SIZE\n",
    "dataloader = DataLoader(ds['train'], batch_size=BATCH_SIZE)\n",
    "\n",
    "clear_cache(lm)\n",
    "module, sae = register_sae(lm, layer=2, l0=100, type='att', width='16k')\n",
    "\n",
    "\n",
    "all_acts = []\n",
    "all_toks = []\n",
    "for i, batch in tqdm(enumerate(dataloader), total=num_batches):\n",
    "    out = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    tok_ids, attn_mask = out['input_ids'], out['attention_mask']\n",
    "    tok_ids = tok_ids[attn_mask[:,0].bool()].cuda()\n",
    "    \n",
    "    try:\n",
    "        lm.forward(tok_ids)\n",
    "    except CustomBreakError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    \n",
    "    sae_inp = module.cache['sae_inp']\n",
    "    \n",
    "    acts = sae.encode(sae_inp, indices=range(0,200))\n",
    "\n",
    "    all_acts.append(acts)\n",
    "    all_toks.append(tok_ids)\n",
    "\n",
    "    if len(all_acts) > num_batches:\n",
    "        break\n",
    "\n",
    "acts = torch.cat(all_acts, dim=0).cpu()\n",
    "toks = tok_strs[np.concatenate([toks.cpu().numpy() for toks in all_toks], axis=0)]\n",
    "\n",
    "from einops import rearrange\n",
    "acts = acts/(rearrange(acts, 'b s a -> ( b s ) a').max(dim=0).values[None,None]+1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import CustomBreakError, register_sae\n",
    "from noa_tools import clear_cache\n",
    "from einops import rearrange\n",
    "\n",
    "def get_acts(lm, layer, l0, sae_type, width='16k', batch_size=200, indices=range(0,100), num_docs=10_000):\n",
    "    \n",
    "\n",
    "    num_batches= num_docs // batch_size\n",
    "    dataloader = DataLoader(ds['train'], batch_size=batch_size)\n",
    "\n",
    "    clear_cache(lm)\n",
    "    module, sae = register_sae(lm, layer=layer, l0=l0, type=sae_type, width='16k')\n",
    "\n",
    "\n",
    "    all_acts = []\n",
    "    all_toks = []\n",
    "    for i, batch in tqdm(enumerate(dataloader), total=num_batches):\n",
    "        out = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        tok_ids, attn_mask = out['input_ids'], out['attention_mask']\n",
    "        tok_ids = tok_ids[attn_mask[:,0].bool()].cuda()\n",
    "        \n",
    "        try:\n",
    "            lm.forward(tok_ids)\n",
    "        except CustomBreakError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "        \n",
    "        sae_inp = module.cache['sae_inp']\n",
    "        \n",
    "        acts = sae.encode(sae_inp, indices=range(0,200))\n",
    "\n",
    "        all_acts.append(acts)\n",
    "        all_toks.append(tok_ids)\n",
    "\n",
    "        if len(all_acts) > num_batches:\n",
    "            break\n",
    "\n",
    "    acts = torch.cat(all_acts, dim=0).cpu()\n",
    "    toks = tok_strs[np.concatenate([toks.cpu().numpy() for toks in all_toks], axis=0)]\n",
    "\n",
    "    \n",
    "    acts = acts/(rearrange(acts, 'b s a -> ( b s ) a').max(dim=0).values[None,None]+1e-10)\n",
    "    \n",
    "    return acts, toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysvelte\n",
    "\n",
    "N_DOCS = -1\n",
    "\n",
    "FEAT_START = 100\n",
    "N_FEATS = 20\n",
    "\n",
    "for FEAT in range(FEAT_START, FEAT_START + N_FEATS):\n",
    "    print(f'Feat {FEAT}')\n",
    "\n",
    "    feat_acts = acts[:N_DOCS,:,FEAT]\n",
    "    feat_toks = toks[:N_DOCS]\n",
    "    feat_mask = feat_acts.max(dim=-1).values > 0\n",
    "    docs = feat_toks[feat_mask].tolist()\n",
    "    feat_acts = feat_acts[feat_mask].cpu().tolist()\n",
    "    pysvelte.WeightedDocs(docs=docs, acts=feat_acts, start=0.8, k=4).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate(input_ids=tok_ids, attention_mask=attn_mask, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Once upon a time there was a giant'\n",
    "\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=50, temperature=1.0, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode([tok_id]) for tok_id in outputs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "GenerationConfig.from_pretrained(\"google/gemma-2b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
